{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model_paired_ae_emma import MultimodalAE, Encoder, Decoder\n",
    "from src.dataset import generate_datasets, concat_datasets\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading paired dataset\n",
      "Loading paired dataset\n"
     ]
    }
   ],
   "source": [
    "# Génération des ensembles de données\n",
    "train_datasets = generate_datasets(suffix='5_diff', type='paired', train=True, test=False)\n",
    "test_datasets = generate_datasets(suffix='5_diff', type='paired', train=False, test=True)\n",
    "\n",
    "\n",
    "# Création des chargeurs de données\n",
    "train_loaders = [DataLoader(dataset, batch_size=32, shuffle=True) for dataset in train_datasets]\n",
    "test_loaders = [DataLoader(dataset, batch_size=32, shuffle=False) for dataset in test_datasets]\n",
    "\n",
    "\n",
    "# Obtenez les dimensions d'entrée à partir des ensembles de données\n",
    "n_inputs1 = train_datasets[0][0][0].shape[0]  # La taille du vecteur de caractéristiques pour la modalité 1\n",
    "n_inputs2 = train_datasets[1][0][0].shape[0]  # La taille du vecteur de caractéristiques pour la modalité 2\n",
    "n_inputs3 = train_datasets[2][0][0].shape[0]  # La taille du vecteur de caractéristiques pour la modalité 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 20   \n",
    "n_hiddens = 256\n",
    "\n",
    "# Création du modèle\n",
    "encoder1 = Encoder(n_inputs1, latent_dims, n_hiddens)\n",
    "encoder2 = Encoder(n_inputs2, latent_dims, n_hiddens)\n",
    "encoder3 = Encoder(n_inputs3, latent_dims, n_hiddens)\n",
    "\n",
    "z1 = encoder1(train_datasets[0][0][0])\n",
    "z2 = encoder2(train_datasets[1][0][0])\n",
    "z3 = encoder3(train_datasets[2][0][0])\n",
    "\n",
    "decoder1 = Decoder(n_inputs1, latent_dims, n_hiddens)\n",
    "decoder2 = Decoder(n_inputs2, latent_dims, n_hiddens)\n",
    "decoder3 = Decoder(n_inputs3, latent_dims, n_hiddens)\n",
    "\n",
    "model1 = MultimodalAE(encoder=encoder1, decoder=decoder1)\n",
    "model2 = MultimodalAE(encoder=encoder2, decoder=decoder2)\n",
    "model1_2 = MultimodalAE(encoder=encoder1, decoder=decoder2)\n",
    "model2_1 = MultimodalAE(encoder=encoder2, decoder=decoder1)\n",
    "model3 = MultimodalAE(encoder=encoder3, decoder=decoder3)\n",
    "model1_3 = MultimodalAE(encoder=encoder1, decoder=decoder3)\n",
    "model3_1 = MultimodalAE(encoder=encoder3, decoder=decoder1)\n",
    "model2_3 = MultimodalAE(encoder=encoder2, decoder=decoder3)\n",
    "model3_2 = MultimodalAE(encoder=encoder3, decoder=decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critère et optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "critere = nn.MSELoss()\n",
    "lr = 0.001\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=lr)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "optimizer1_2 = torch.optim.Adam(model1_2.parameters(), lr=lr)\n",
    "optimizer2_1 = torch.optim.Adam(model2_1.parameters(), lr=lr)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=lr)\n",
    "optimizer1_3 = torch.optim.Adam(model1_3.parameters(), lr=lr)\n",
    "optimizer3_1 = torch.optim.Adam(model3_1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0 -> loss totale : 0.872  loss1 : 0.4587  loss3 : 0.4133  loss1_3 : 1.1662  loss3_1 : 1.2347\n",
      " Epoch 1 -> loss totale : 0.7156  loss1 : 0.4271  loss3 : 0.2885  loss1_3 : 1.1052  loss3_1 : 1.2175\n",
      " Epoch 2 -> loss totale : 0.6793  loss1 : 0.3718  loss3 : 0.3074  loss1_3 : 1.0663  loss3_1 : 1.1322\n",
      " Epoch 3 -> loss totale : 0.5986  loss1 : 0.3565  loss3 : 0.2421  loss1_3 : 1.0444  loss3_1 : 1.1044\n",
      " Epoch 4 -> loss totale : 0.5453  loss1 : 0.2999  loss3 : 0.2454  loss1_3 : 1.126  loss3_1 : 1.2976\n",
      " Epoch 5 -> loss totale : 0.4965  loss1 : 0.285  loss3 : 0.2115  loss1_3 : 1.1227  loss3_1 : 1.1562\n",
      " Epoch 6 -> loss totale : 0.4808  loss1 : 0.2808  loss3 : 0.2  loss1_3 : 1.1729  loss3_1 : 1.3684\n",
      " Epoch 7 -> loss totale : 0.4317  loss1 : 0.2535  loss3 : 0.1782  loss1_3 : 1.2115  loss3_1 : 1.4\n",
      " Epoch 8 -> loss totale : 0.4174  loss1 : 0.247  loss3 : 0.1703  loss1_3 : 1.2389  loss3_1 : 1.2847\n",
      " Epoch 9 -> loss totale : 0.4088  loss1 : 0.2453  loss3 : 0.1635  loss1_3 : 1.2331  loss3_1 : 1.3686\n",
      " Epoch 10 -> loss totale : 0.3606  loss1 : 0.2085  loss3 : 0.1521  loss1_3 : 1.2445  loss3_1 : 1.4106\n",
      " Epoch 11 -> loss totale : 0.3418  loss1 : 0.205  loss3 : 0.1368  loss1_3 : 1.1311  loss3_1 : 1.2762\n",
      " Epoch 12 -> loss totale : 0.3458  loss1 : 0.2113  loss3 : 0.1345  loss1_3 : 1.2498  loss3_1 : 1.3851\n",
      " Epoch 13 -> loss totale : 0.3279  loss1 : 0.1943  loss3 : 0.1335  loss1_3 : 1.462  loss3_1 : 1.4074\n",
      " Epoch 14 -> loss totale : 0.3136  loss1 : 0.1927  loss3 : 0.1209  loss1_3 : 1.3817  loss3_1 : 1.4205\n",
      " Epoch 15 -> loss totale : 0.3236  loss1 : 0.1986  loss3 : 0.125  loss1_3 : 1.4321  loss3_1 : 1.4836\n",
      " Epoch 16 -> loss totale : 0.301  loss1 : 0.1868  loss3 : 0.1142  loss1_3 : 1.456  loss3_1 : 1.4483\n",
      " Epoch 17 -> loss totale : 0.2851  loss1 : 0.1732  loss3 : 0.1119  loss1_3 : 1.5062  loss3_1 : 1.5107\n",
      " Epoch 18 -> loss totale : 0.2829  loss1 : 0.1713  loss3 : 0.1116  loss1_3 : 1.5142  loss3_1 : 1.5229\n",
      " Epoch 19 -> loss totale : 0.2819  loss1 : 0.1747  loss3 : 0.1073  loss1_3 : 1.4113  loss3_1 : 1.4808\n",
      " Epoch 20 -> loss totale : 0.2734  loss1 : 0.1722  loss3 : 0.1012  loss1_3 : 1.3652  loss3_1 : 1.5347\n",
      " Epoch 21 -> loss totale : 0.2736  loss1 : 0.1685  loss3 : 0.105  loss1_3 : 1.5068  loss3_1 : 1.5878\n",
      " Epoch 22 -> loss totale : 0.2687  loss1 : 0.1708  loss3 : 0.098  loss1_3 : 1.4737  loss3_1 : 1.5077\n",
      " Epoch 23 -> loss totale : 0.2467  loss1 : 0.1517  loss3 : 0.0951  loss1_3 : 1.514  loss3_1 : 1.5207\n",
      " Epoch 24 -> loss totale : 0.258  loss1 : 0.1571  loss3 : 0.1009  loss1_3 : 1.4258  loss3_1 : 1.4347\n",
      " Epoch 25 -> loss totale : 0.2455  loss1 : 0.149  loss3 : 0.0965  loss1_3 : 1.4981  loss3_1 : 1.5272\n",
      " Epoch 26 -> loss totale : 0.2497  loss1 : 0.1517  loss3 : 0.098  loss1_3 : 1.6223  loss3_1 : 1.5204\n",
      " Epoch 27 -> loss totale : 0.2307  loss1 : 0.143  loss3 : 0.0876  loss1_3 : 1.5285  loss3_1 : 1.5975\n",
      " Epoch 28 -> loss totale : 0.2417  loss1 : 0.1536  loss3 : 0.0881  loss1_3 : 1.4681  loss3_1 : 1.5139\n",
      " Epoch 29 -> loss totale : 0.2291  loss1 : 0.1406  loss3 : 0.0885  loss1_3 : 1.6501  loss3_1 : 1.4958\n",
      " Epoch 30 -> loss totale : 0.2134  loss1 : 0.1324  loss3 : 0.0811  loss1_3 : 1.4168  loss3_1 : 1.4488\n",
      " Epoch 31 -> loss totale : 0.2273  loss1 : 0.1419  loss3 : 0.0854  loss1_3 : 1.5465  loss3_1 : 1.5558\n",
      " Epoch 32 -> loss totale : 0.2256  loss1 : 0.1398  loss3 : 0.0857  loss1_3 : 1.595  loss3_1 : 1.509\n",
      " Epoch 33 -> loss totale : 0.2146  loss1 : 0.13  loss3 : 0.0846  loss1_3 : 1.6412  loss3_1 : 1.5859\n",
      " Epoch 34 -> loss totale : 0.2182  loss1 : 0.1378  loss3 : 0.0804  loss1_3 : 1.5942  loss3_1 : 1.4456\n",
      " Epoch 35 -> loss totale : 0.2124  loss1 : 0.1284  loss3 : 0.084  loss1_3 : 1.6459  loss3_1 : 1.4438\n",
      " Epoch 36 -> loss totale : 0.1987  loss1 : 0.1206  loss3 : 0.0781  loss1_3 : 1.5255  loss3_1 : 1.4261\n",
      " Epoch 37 -> loss totale : 0.202  loss1 : 0.1191  loss3 : 0.0829  loss1_3 : 1.618  loss3_1 : 1.4858\n",
      " Epoch 38 -> loss totale : 0.2039  loss1 : 0.1292  loss3 : 0.0747  loss1_3 : 1.6473  loss3_1 : 1.5324\n",
      " Epoch 39 -> loss totale : 0.1997  loss1 : 0.121  loss3 : 0.0788  loss1_3 : 1.7126  loss3_1 : 1.4604\n",
      " Epoch 40 -> loss totale : 0.1902  loss1 : 0.1166  loss3 : 0.0736  loss1_3 : 1.6335  loss3_1 : 1.5292\n",
      " Epoch 41 -> loss totale : 0.1894  loss1 : 0.1173  loss3 : 0.0722  loss1_3 : 1.669  loss3_1 : 1.4768\n",
      " Epoch 42 -> loss totale : 0.1901  loss1 : 0.1164  loss3 : 0.0737  loss1_3 : 1.6787  loss3_1 : 1.5181\n",
      " Epoch 43 -> loss totale : 0.1892  loss1 : 0.1156  loss3 : 0.0737  loss1_3 : 1.6895  loss3_1 : 1.4415\n",
      " Epoch 44 -> loss totale : 0.1886  loss1 : 0.1153  loss3 : 0.0733  loss1_3 : 1.6956  loss3_1 : 1.5112\n",
      " Epoch 45 -> loss totale : 0.1785  loss1 : 0.1075  loss3 : 0.071  loss1_3 : 1.5235  loss3_1 : 1.3685\n",
      " Epoch 46 -> loss totale : 0.1783  loss1 : 0.1075  loss3 : 0.0707  loss1_3 : 1.762  loss3_1 : 1.4948\n",
      " Epoch 47 -> loss totale : 0.1796  loss1 : 0.1101  loss3 : 0.0695  loss1_3 : 1.6256  loss3_1 : 1.4655\n",
      " Epoch 48 -> loss totale : 0.1724  loss1 : 0.1047  loss3 : 0.0676  loss1_3 : 1.6237  loss3_1 : 1.4759\n",
      " Epoch 49 -> loss totale : 0.1677  loss1 : 0.1002  loss3 : 0.0675  loss1_3 : 1.7072  loss3_1 : 1.4521\n",
      " Epoch 50 -> loss totale : 0.1678  loss1 : 0.1037  loss3 : 0.0642  loss1_3 : 1.5845  loss3_1 : 1.3467\n",
      " Epoch 51 -> loss totale : 0.1713  loss1 : 0.1031  loss3 : 0.0682  loss1_3 : 1.5525  loss3_1 : 1.3586\n",
      " Epoch 52 -> loss totale : 0.1646  loss1 : 0.0984  loss3 : 0.0663  loss1_3 : 1.5523  loss3_1 : 1.3608\n",
      " Epoch 53 -> loss totale : 0.1614  loss1 : 0.0943  loss3 : 0.0671  loss1_3 : 1.7744  loss3_1 : 1.5228\n",
      " Epoch 54 -> loss totale : 0.1582  loss1 : 0.0929  loss3 : 0.0652  loss1_3 : 1.9056  loss3_1 : 1.4589\n",
      " Epoch 55 -> loss totale : 0.1634  loss1 : 0.0954  loss3 : 0.068  loss1_3 : 1.8829  loss3_1 : 1.5097\n",
      " Epoch 56 -> loss totale : 0.1515  loss1 : 0.0887  loss3 : 0.0628  loss1_3 : 1.7439  loss3_1 : 1.5846\n",
      " Epoch 57 -> loss totale : 0.1522  loss1 : 0.0904  loss3 : 0.0618  loss1_3 : 1.8013  loss3_1 : 1.3725\n",
      " Epoch 58 -> loss totale : 0.1394  loss1 : 0.0826  loss3 : 0.0568  loss1_3 : 1.9145  loss3_1 : 1.5127\n",
      " Epoch 59 -> loss totale : 0.1446  loss1 : 0.0842  loss3 : 0.0603  loss1_3 : 1.8594  loss3_1 : 1.5755\n",
      " Epoch 60 -> loss totale : 0.1477  loss1 : 0.0833  loss3 : 0.0644  loss1_3 : 1.715  loss3_1 : 1.4729\n",
      " Epoch 61 -> loss totale : 0.1361  loss1 : 0.081  loss3 : 0.0551  loss1_3 : 1.6217  loss3_1 : 1.4544\n",
      " Epoch 62 -> loss totale : 0.1486  loss1 : 0.0876  loss3 : 0.061  loss1_3 : 1.7388  loss3_1 : 1.4194\n",
      " Epoch 63 -> loss totale : 0.1409  loss1 : 0.0824  loss3 : 0.0585  loss1_3 : 1.7789  loss3_1 : 1.4823\n",
      " Epoch 64 -> loss totale : 0.1422  loss1 : 0.0851  loss3 : 0.0571  loss1_3 : 1.9209  loss3_1 : 1.5232\n",
      " Epoch 65 -> loss totale : 0.1351  loss1 : 0.0798  loss3 : 0.0553  loss1_3 : 1.6993  loss3_1 : 1.5267\n",
      " Epoch 66 -> loss totale : 0.1348  loss1 : 0.0782  loss3 : 0.0565  loss1_3 : 1.8832  loss3_1 : 1.4835\n",
      " Epoch 67 -> loss totale : 0.1326  loss1 : 0.0778  loss3 : 0.0548  loss1_3 : 1.8705  loss3_1 : 1.5125\n",
      " Epoch 68 -> loss totale : 0.137  loss1 : 0.078  loss3 : 0.0589  loss1_3 : 1.7793  loss3_1 : 1.4957\n",
      " Epoch 69 -> loss totale : 0.1343  loss1 : 0.0801  loss3 : 0.0543  loss1_3 : 1.8721  loss3_1 : 1.4628\n",
      " Epoch 70 -> loss totale : 0.1299  loss1 : 0.078  loss3 : 0.052  loss1_3 : 2.0138  loss3_1 : 1.6183\n",
      " Epoch 71 -> loss totale : 0.1337  loss1 : 0.0776  loss3 : 0.0561  loss1_3 : 1.9292  loss3_1 : 1.5567\n",
      " Epoch 72 -> loss totale : 0.1333  loss1 : 0.0785  loss3 : 0.0548  loss1_3 : 1.7096  loss3_1 : 1.3966\n",
      " Epoch 73 -> loss totale : 0.1269  loss1 : 0.0737  loss3 : 0.0533  loss1_3 : 1.7785  loss3_1 : 1.4087\n",
      " Epoch 74 -> loss totale : 0.1242  loss1 : 0.071  loss3 : 0.0532  loss1_3 : 1.6549  loss3_1 : 1.3205\n",
      " Epoch 75 -> loss totale : 0.1159  loss1 : 0.0671  loss3 : 0.0488  loss1_3 : 1.7887  loss3_1 : 1.3851\n",
      " Epoch 76 -> loss totale : 0.1165  loss1 : 0.0686  loss3 : 0.0479  loss1_3 : 1.8354  loss3_1 : 1.4187\n",
      " Epoch 77 -> loss totale : 0.1198  loss1 : 0.0657  loss3 : 0.054  loss1_3 : 1.6899  loss3_1 : 1.3599\n",
      " Epoch 78 -> loss totale : 0.1163  loss1 : 0.0686  loss3 : 0.0478  loss1_3 : 1.855  loss3_1 : 1.5075\n",
      " Epoch 79 -> loss totale : 0.1142  loss1 : 0.0645  loss3 : 0.0497  loss1_3 : 1.8303  loss3_1 : 1.5935\n",
      " Epoch 80 -> loss totale : 0.1181  loss1 : 0.0701  loss3 : 0.048  loss1_3 : 1.8069  loss3_1 : 1.4617\n",
      " Epoch 81 -> loss totale : 0.1119  loss1 : 0.0645  loss3 : 0.0474  loss1_3 : 2.0248  loss3_1 : 1.5318\n",
      " Epoch 82 -> loss totale : 0.1151  loss1 : 0.0638  loss3 : 0.0513  loss1_3 : 1.8397  loss3_1 : 1.5113\n",
      " Epoch 83 -> loss totale : 0.1094  loss1 : 0.0638  loss3 : 0.0456  loss1_3 : 1.7391  loss3_1 : 1.3963\n",
      " Epoch 84 -> loss totale : 0.1102  loss1 : 0.066  loss3 : 0.0443  loss1_3 : 1.7811  loss3_1 : 1.4955\n",
      " Epoch 85 -> loss totale : 0.1082  loss1 : 0.0614  loss3 : 0.0468  loss1_3 : 1.8587  loss3_1 : 1.4693\n",
      " Epoch 86 -> loss totale : 0.1005  loss1 : 0.0567  loss3 : 0.0438  loss1_3 : 2.0577  loss3_1 : 1.6128\n",
      " Epoch 87 -> loss totale : 0.0993  loss1 : 0.0572  loss3 : 0.0421  loss1_3 : 1.7402  loss3_1 : 1.5147\n",
      " Epoch 88 -> loss totale : 0.1006  loss1 : 0.0585  loss3 : 0.0421  loss1_3 : 1.7934  loss3_1 : 1.5071\n",
      " Epoch 89 -> loss totale : 0.1003  loss1 : 0.0596  loss3 : 0.0407  loss1_3 : 1.8098  loss3_1 : 1.5963\n",
      " Epoch 90 -> loss totale : 0.1044  loss1 : 0.0639  loss3 : 0.0405  loss1_3 : 1.8117  loss3_1 : 1.5079\n",
      " Epoch 91 -> loss totale : 0.0977  loss1 : 0.0563  loss3 : 0.0414  loss1_3 : 1.9969  loss3_1 : 1.6432\n",
      " Epoch 92 -> loss totale : 0.0998  loss1 : 0.0594  loss3 : 0.0404  loss1_3 : 1.8405  loss3_1 : 1.5676\n",
      " Epoch 93 -> loss totale : 0.0959  loss1 : 0.0564  loss3 : 0.0396  loss1_3 : 1.8947  loss3_1 : 1.6068\n",
      " Epoch 94 -> loss totale : 0.1011  loss1 : 0.0593  loss3 : 0.0417  loss1_3 : 1.7738  loss3_1 : 1.5289\n",
      " Epoch 95 -> loss totale : 0.0983  loss1 : 0.0566  loss3 : 0.0418  loss1_3 : 1.6508  loss3_1 : 1.5364\n",
      " Epoch 96 -> loss totale : 0.0957  loss1 : 0.0583  loss3 : 0.0374  loss1_3 : 1.8724  loss3_1 : 1.5086\n",
      " Epoch 97 -> loss totale : 0.0906  loss1 : 0.0523  loss3 : 0.0384  loss1_3 : 1.6549  loss3_1 : 1.5137\n",
      " Epoch 98 -> loss totale : 0.0902  loss1 : 0.0492  loss3 : 0.0409  loss1_3 : 1.8275  loss3_1 : 1.5486\n",
      " Epoch 99 -> loss totale : 0.0911  loss1 : 0.0531  loss3 : 0.038  loss1_3 : 1.7607  loss3_1 : 1.5583\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    model3.train()\n",
    "    model1_3.train()\n",
    "    model3_1.train()\n",
    "    for (x1, _), (x2, _), (x3, y) in zip(*train_loaders):\n",
    "\n",
    "        if x1.size(0) != 32 or x3.size(0) != 32 or y.size(0) != 32:\n",
    "            continue\n",
    "\n",
    "       # Forward pass\n",
    "        o1, o3, o1_3, o3_1 = model1(x1), model3(x3), model1_3(x1), model3_1(x3)\n",
    "        # loss = critere(o1, x1) + critere(o3, x3) + 0*critere(o1_3, x3) + 0*critere(o3_1, x1)\n",
    "        loss = critere(o1, x1) + critere(o3, x3) + critere(o1_3, x3) + critere(o3_1, x1)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        # optimizer1_3.zero_grad()\n",
    "        # optimizer3_1.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer1.step()\n",
    "        optimizer3.step()\n",
    "        # optimizer1_3.step()\n",
    "        # optimizer3_1.step()\n",
    "        \n",
    "\n",
    "    print(f' Epoch {epoch} -> loss totale :', round(loss.item(), 4), \" loss1 :\", round(critere(o1, x1).item(), 4), \" loss3 :\", round(critere(o3, x3).item(), 4), \" loss1_3 :\", round(critere(o1_3, x3).item(), 4), \" loss3_1 :\", round(critere(o3_1, x1).item(), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.4732488904680525\n",
      " loss1 : 0.42336663603782654  loss3 : 0.9735544919967651  loss1_3 : 1.0054936408996582  loss3_1 : 1.0614235401153564\n"
     ]
    }
   ],
   "source": [
    "model1.eval()\n",
    "model3.eval()\n",
    "model1_3.eval()\n",
    "model3_1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for (x1, _), (x2, _), (x3, y) in zip(*test_loaders):\n",
    "        o1, o3, o1_3, o3_1 = model1(x1), model3(x3), model1_3(x1), model3_1(x3)\n",
    "        loss = critere(o1, x1) + critere(o3, x3) + critere(o1_3, x3) + critere(o3_1, x1)\n",
    "        total_loss += loss.item()\n",
    "    print(f'Test Loss: {total_loss / len(test_loaders[0])}')\n",
    "    print(\" loss1 :\", critere(o1, x1).item(), \" loss3 :\", critere(o3, x3).item(), \" loss1_3 :\", critere(o1_3, x3).item(), \" loss3_1 :\", critere(o3_1, x1).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
